{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4XyJGI2BPFEiX9Y67+TA6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LuijzrG6UKI8"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import BertModel, BertTokenizer\n","\n","class PolyEncoder(nn.Module):\n","    def __init__(self, encoder_name, poly_m=16):\n","        super(PolyEncoder, self).__init__()\n","        self.encoder = BertModel.from_pretrained(encoder_name)\n","        self.poly_m = poly_m\n","        self.poly_code_embeddings = nn.Embedding(self.poly_m, self.encoder.config.hidden_size)\n","        self.poly_code_embeddings.weight.data.normal_(mean=0.0, std=self.encoder.config.initializer_range)\n","\n","    def forward(self, input_ids, attention_mask):\n","        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n","        encoder_hidden_states = encoder_outputs.last_hidden_state\n","\n","        # Poly-encoder context embeddings\n","        poly_codes = self.poly_code_embeddings.weight.unsqueeze(0).expand(input_ids.size(0), -1, -1)\n","        poly_mask = torch.ones(poly_codes.size()[:2], device=poly_codes.device)\n","\n","        # Attention between poly codes and encoder hidden states\n","        poly_contexts = torch.bmm(poly_codes, encoder_hidden_states.transpose(1, 2))\n","        return poly_contexts, poly_mask\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  # 한국어를 잘 해주는 bert를 찾아봐야 할 것 같음 (수정 예정)\n","poly_encoder = PolyEncoder(\"bert-base-uncased\")  # 한국어를 잘 해주는 bert를 찾아봐야 할 것 같음 (수정 예정)"]},{"cell_type":"code","source":["from peft import LoraConfig, get_peft_model\n","\n","# LoraConfig 설정\n","lora_config = LoraConfig(\n","    r=16,  # Low-rank dimension\n","    lora_alpha=32,  # Scaling factor\n","    lora_dropout=0.1,  # Dropout rate\n","    bias=\"none\"  # Bias setting\n",")\n","\n","# PEFT 적용\n","model = get_peft_model(model, lora_config)"],"metadata":{"id":"7HmzCuVnSnWG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 예시 데이터\n","examples = [\n","    {\"context\": \"여기에 문서 내용\", \"query\": \"질문 내용\", \"summary\": \"정답 요약\"},\n","    # 추가 데이터...\n","]\n","\n","# 데이터 전처리\n","inputs = [tokenizer(ex[\"context\"], ex[\"query\"], return_tensors=\"pt\", truncation=True) for ex in examples]\n","labels = [tokenizer(ex[\"summary\"], return_tensors=\"pt\", truncation=True) for ex in examples]"],"metadata":{"id":"B4oovT4RSqiL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","llama_model_name = \"vaiv/llamion-14b-base\"\n","llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_name)\n","llama_model = AutoModelForCausalLM.from_pretrained(llama_model_name)"],"metadata":{"id":"BByc2vXBUejI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PolyLlamaModel(nn.Module):\n","    def __init__(self, encoder_name, decoder_name, poly_m=16):\n","        super(PolyLlamaModel, self).__init__()\n","        self.encoder = PolyEncoder(encoder_name, poly_m)\n","        self.decoder = AutoModelForCausalLM.from_pretrained(decoder_name)\n","\n","    def forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask):\n","        # Encoder\n","        encoder_contexts, poly_mask = self.encoder(input_ids, attention_mask)\n","\n","        # Decoder\n","        decoder_outputs = self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask,\n","                                       encoder_hidden_states=encoder_contexts, encoder_attention_mask=poly_mask)\n","\n","        return decoder_outputs\n","\n","# 모델 초기화\n","model = PolyLlamaModel(\"bert-base-uncased\", \"vaiv/llamion-14b-base\")"],"metadata":{"id":"6DWfI5ghUj6T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",   # 수정 필요\n","    num_train_epochs=3,\n","    per_device_train_batch_size=2,\n","    save_steps=10_000,\n","    save_total_limit=2,\n",")\n","\n","# Trainer 설정\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=inputs,  # 학습 데이터셋(확실하지 않음, 다시 확인해봐야 함)\n","    eval_dataset=labels,    # 정답 데이터셋(확실하지 않음, 다시 확인해봐야 함)\n",")\n","\n","# 모델 학습\n","trainer.train()\n","\n","# 모델 평가\n","results = trainer.evaluate()\n","print(f\"Evaluation results: {results}\")"],"metadata":{"id":"UGuIPTkZUlzz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 평가 함수\n","def evaluate_model(model, tokenizer, dataset):\n","    model.eval()\n","    correct = 0\n","    total = len(dataset)\n","\n","    for example in dataset:\n","        inputs = tokenizer(example[\"context\"], example[\"query\"], return_tensors=\"pt\")\n","        with torch.no_grad():\n","            outputs = model.generate(inputs.input_ids)\n","        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","        if prediction == example[\"summary\"]:\n","            correct += 1\n","\n","    accuracy = correct / total\n","    return accuracy\n","\n","# 평가 실행\n","accuracy = evaluate_model(model, tokenizer, examples)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")"],"metadata":{"id":"T7K80OBxSxPZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## bash 파일에 넣어야 할 것들 (모델 추출을 위함)\n","- huggingface-cli login\n","- transformers-cli repo create my-awesome-model\n","- transformers-cli repo upload --repo my-awesome-model --path ./results"],"metadata":{"id":"Ux8PKufGS5w0"}}]}